@node Getting started
@chapter Getting started

@menu
* Installation::                           How to install Sherpa
* Running Sherpa::                         How to run the event generator
@end menu

@node Installation
@section Installation

Sherpa is distributed as a tarred and gzipped file named
@code{SHERPA-MC-@value{VERSION}.tar.gz}, and can be unpacked in the current
working directory with

@example
@code{ tar -zxf SHERPA-MC-@value{VERSION}.tar.gz}
@end example

Alternatively, it can also be accessed via SVN through the location specified
on the download page. In that case, before continuing, it is necessary to
construct the build scripts by running @code{autoreconf -i} once after checking
out the SVN working copy.

To guarantee successful installation, the following tools should be
available on the system:
@itemize @bullet
  @item C++ and Fortran compilers (e.g. from the gcc suite)
  @item make
  @item @url{http://www.sqlite.org/,,SQLite 3} 
    (including the -dev package if installed through a package manager)
@end itemize
If SQLite is installed in a non-standard location, please specify
the installation path using option @option{--with-sqlite3=/path/to/sqlite}.
If SQLite is not installed on your system, the Sherpa configure script provides 
the fallback option of installing it into the same directory as Sherpa itself.
To do so, please run configure with option @option{--with-sqlite3=install}
(This may not work if you are cross-compiling using @option{--host}. 
In this case, please @url{http://www.sqlite.org/download.html,,install SQLite}
by yourself and reconfigure using @option{--with-sqlite3=/path/to/sqlite}).

Compilation and installation proceed through the following commands:

@example
@code{ ./configure}

@code{ make install}
@end example

If not specified differently, the directory structure after installation is organized as follows 

@table @code
@item $(prefix)/bin
Sherpa executeable and scripts

@item $(prefix)/include
headers for process library compilation

@item $(prefix)/lib
basic libraries

@item $(prefix)/share
PDFs, Decaydata, fallback run cards
@end table

The installation directory @code{$(prefix)} can be specified by using the
@code{./configure --prefix /path/to/installation/target} directive and defaults
to the current working directory.

If Sherpa has to be moved to a different directory after the installation,
one has to set the following environment variables for each run:
@itemize @bullet
  @item @code{SHERPA_INCLUDE_PATH=$newprefix/include/SHERPA-MC}
  @item @code{SHERPA_SHARE_PATH=$newprefix/share/SHERPA-MC}
  @item @code{SHERPA_LIBRARY_PATH=$newprefix/lib/SHERPA-MC}
  @item @code{LD_LIBRARY_PATH=$SHERPA_LIBRARY_PATH:$LD_LIBRARY_PATH}
@end itemize

Sherpa can be interfaced with various external packages, e.g. @uref{http://lcgapp.cern.ch/project/simu/HepMC/,,HepMC},
for event output, or @uref{https://lhapdf.hepforge.org/,,LHAPDF}, for PDFs. For this to work, the user has
to pass the appropriate commands to the configure step. This is achieved 
as shown below:

@example
@code{./configure --enable-hepmc2=/path/to/hepmc2 --enable-lhapdf=/path/to/lhapdf}
@end example

Here, the paths have to point to the top level installation directories of
the external packages, i.e. the ones containing the @code{lib/}, @code{share/},
... subdirectories.

For a complete list of possible configuration options run
@samp{./configure --help}.

@c If you want to use the built-in interface to Lund fragmentation and hadron
@c decays, you have to compile with Pythia support by specifying the
@c @code{--enable-pythia} option without any argument.

The Sherpa package has successfully been compiled, installed and
tested on SuSE, RedHat / Scientific Linux and Debian / Ubuntu Linux systems 
using the GNU C++ compiler versions 3.2, 3.3, 3.4, and 4.x as
well as on Mac OS X 10 using the GNU C++ compiler version 4.0. In all cases the 
GNU FORTRAN compiler g77 or gfortran has been employed.

If you have multiple compilers installed on your system, you can use shell 
environment variables to specify which of these are to be used. A list of 
the available variables is printed with
@example
@code{./configure --help}
@end example
in the Sherpa top level directory and looking at the last lines. Depending 
on the shell you are using, you can set these variables e.g. with export 
(bash) or setenv (csh).
Examples:
@example
@code{export CXX=g++-3.4
export CC=gcc-3.4
export CPP=cpp-3.4}
@end example

@subsubheading Installation on Cray XE6 / XK7

Sherpa has been installed successfully on Cray XE6 and Cray XK7.
The following configure command should be used
@example
./configure <your options> --enable-mpi --host=i686-pc-linux CC=CC CXX=CC FC='ftn -fPIC' LDFLAGS=-dynamic
@end example
Sherpa can then be run with
@example
aprun -n <nofcores> <prefix>/bin/Sherpa -lrun.log
@end example
The modularity of the code requires setting the environment variable @option{CRAY_ROOTFS}, 
cf. the Cray system documentation.

@subsubheading Installation on IBM BlueGene/Q

Sherpa has been installed successfully on an IBM BlueGene/Q system. 
The following configure command should be used
@example
./configure <your options> --enable-mpi --host=powerpc64-bgq-linux CC=mpic++ CXX=mpic++ FC='mpif90 -fPIC -funderscoring' LDFLAGS=-dynamic
@end example
Sherpa can then be run with
@example
qsub -A <account> -n <nofcores> -t 60 --mode c16 <prefix>/bin/Sherpa -lrun.log
@end example

@subsubheading MacOS Installation

Since it is more complicated to set up the necessary compiler environment
on a Mac we recommend using a package manager to install Sherpa and its
dependencies. David Hall is hosting a repository for Homebrew packages at:
@uref{http://davidchall.github.io/homebrew-hep/,,http://davidchall.github.io/homebrew-hep/}


In case you are compiling yourself, please be aware of
the following issues which have come up on Mac installations before:
@itemize @bullet
@item On 10.4 and 10.5 only gfortran is supported, and you will 
have to install it e.g. from HPC
@item If you want to reconfigure, i.e. run the command @code{autoreconf} 
or @code{(g)libtoolize}, you have to make sure that you have a recent version 
of GNU libtool (>=1.5.22 has been tested). Don’t confuse this with 
the native non-GNU libtool which is installed in @code{/usr/bin/libtool} and 
of no use! Also make sure that your autools (autoconf >= 2.61, 
automake >= 1.10 have been tested) are of recent versions. All this should 
not be necessary though, if you only run @code{configure}.
@item Make sure that you don’t have two versions of g++ and libstdc++ 
installed and being used inconsistently. This appeared e.g. when the gcc 
suite was installed through Fink to get gfortran. This caused Sherpa to 
use the native MacOS compilers but link the libstdc++ from Fink (which 
is located in /sw/lib). You can find out which libraries are used by Sherpa 
by running @code{otool -L bin/Sherpa}
@end itemize


@node Running Sherpa
@section Running Sherpa


The @code{Sherpa} executable resides in the directory @code{<prefix>/bin/}
where @code{<prefix>} denotes the path to the Sherpa installation
directory. The way a particular simulation will be accomplished is
defined by several parameters, which can all be listed in a
common file, or data card (Parameters can be
alternatively specified on the command line; more details are given
in @ref{Input structure}). 
This steering file is called @code{Run.dat} and some example setups
(i.e. @code{Run.dat} files) are distributed with the current version
of Sherpa. They can be found in the directory 
@code{<prefix>/share/SHERPA-MC/Examples/}, and descriptions of some of 
their key features can be found in the section @ref{Examples}.

@strong{Please note:} It is not in general possible to reuse run 
cards from previous Sherpa versions. Often there are small changes 
in the parameter syntax of the run cards from 
one version to the next. 
These changes are documented in our manuals. In addition, 
always use the newer Hadron.dat and Decaydata directories 
(and reapply any changes which you might have applied to the old ones),
see @ref{Hadron decays}.
 
The very first step in running Sherpa 
is therefore to adjust all parameters to the needs of the
desired simulation. The details for doing this properly are given in
@ref{Parameters}. In this section, the focus is on the main
issues for a successful operation of Sherpa. This is illustrated by
discussing and referring to the parameter settings that come in the run card 
@code{./Examples/V_plus_Jets/LHC_ZJets/Run.dat}, cf. @ref{LHC_ZJets}.
This is a simple run card
created to show the basics of how to operate Sherpa. @strong{It should be 
stressed that this run-card relies on many of Sherpa's default settings,
and, as such, you should understand those settings before using it to 
look at physics.} For more information on the settings and parameters in 
Sherpa, see @ref{Parameters}, and for more 
examples see the @ref{Examples} section.

@anchor{Process selection and initialization}
@subsection Process selection and initialization

Central to any Monte Carlo simulation is the choice of the hard
processes that initiate the events. These hard processes are
described by matrix elements. In Sherpa,
the selection of processes happens in the @code{(processes)}
part of the steering file.
Only a few 
@iftex 
$2\to2$ 
@end iftex
@ifnottex 
2->2 
@end ifnottex 
reactions have been hard-coded. They are available in the EXTRA_XS module.
The more usual way to compute matrix elements is to employ one of Sherpa's
automated tree-level generators, AMEGIC++ and Comix, see @ref{Basic structure}.
If no matrix-element generator is selected, using the @ref{ME_SIGNAL_GENERATOR}
tag, then Sherpa will use whichever generator is capable of calculating the 
process, checking Comix first, then AMEGIC++ and then EXTRA_XS. Therefore, 
for some processes, several of the options are used. In this example, 
however, all processes will be calculated by Comix.

To begin with the example, the Sherpa run has to be started by changing
into the @code{<prefix>/share/SHERPA-MC/Examples/V_plus_Jets/LHC_ZJets/}
directory and executing

@example
@code{<prefix>/bin/Sherpa} 
@end example

You may also run from an arbitrary directory, employing
@code{<prefix>/bin/Sherpa PATH=<prefix>/share/SHERPA-MC/Examples/V_plus_Jets/LHC_ZJets}. 
In the example, the keyword @code{PATH} is specified by an absolute path. 
It may also be specified relative to the current working directory. If it is
not specified at all or it is omitted, the current working directory
is understood.

For good book-keeping, it is highly recommended to reserve different
subdirectories for different simulations as is demonstrated with
the example setups. 

If AMEGIC++ is used, Sherpa requires an initialization run, where 
C++ source code is written to disk. This code must be compiled into dynamic libraries 
by the user by running the @code{makelibs} script in the working directory.
Alternatively, if @uref{http://www.scons.org/,,scons} is installed,
you may invoke @kbd{<prefix>/bin/make2scons} and run @kbd{scons install}.
After this step Sherpa is run again for the actual cross section integrations and event generation.
For more information on and examples of how to run Sherpa using AMEGIC++, see 
@ref{Running Sherpa with AMEGIC++}.

If the internal hard-coded matrix elements or Comix are used,
and AMEGIC++ is not, an initialization run is not needed, and Sherpa will 
calculate the cross sections and generate events during the first run.

As the cross sections are integrated, the
integration over phase space is optimized to arrive at an
efficient event generation.
Subsequently events are generated if @code{EVENTS} was specified
either on the command line or added to the @code{(run)} section
in the @code{Run.dat} file. 

The generated events are not stored into a file by
default; for details on how to store the events see 
@ref{Event output formats}. Note that the computational effort to
go through this procedure of generating, compiling and integrating the
matrix elements of the hard processes depends on the complexity of the
parton-level final states. For low multiplicities (
@iftex 
$2\to 2,3,4$ 
@end iftex 
@ifnottex 
2->2,3,4 
@end ifnottex
), of course, it can be followed instantly.

@anchor{Results directory}
Usually more than one generation run is wanted. As long as the
parameters that affect the matrix-element integration are not changed,
it is advantageous to store the cross sections obtained during the
generation run for later use. This saves CPU time especially for large
final-state multiplicities of the matrix elements. Per default, Sherpa 
stores these integration results in a directory called @code{Results/}.
The name of the output directory can be customised via 

@example
@code{<prefix>/bin/Sherpa RESULT_DIRECTORY=<result>/}
@end example

see @ref{RESULT_DIRECTORY}. The storage of the integration results can 
be prevented by either using 

@example
@code{<prefix>/bin/Sherpa GENERATE_RESULT_DIRECTORY=0}
@end example

or the command line option @option{-g} can be invoked, see @ref{Command line}. 


If physics parameters change, the cross sections have to be recomputed. 
The new results should either be stored in a new directory or the 
@code{<result>} directory may be re-used once it has been emptied.
Parameters which require a recomputation are any parameters affecting 
the @ref{Model Parameters}, @ref{Matrix Elements} or @ref{Selectors}.
Standard examples are changing the magnitude of couplings,
renormalisation or factorisation scales, changing the PDF or
centre-of-mass energy, or, applying different cuts at the parton
level. If unsure whether a recomputation is required, a simple
test is to remove the
@code{RESULT_DIRECTORY} option from the run command and check
whether the new integration numbers (statistically) comply with the
stored ones.

A warning on the validity of the
process libraries is in order here: it is absolutely mandatory to
generate new library files, whenever the physics model is altered,
i.e. particles are added or removed and hence new or existing
diagrams may or may not anymore contribute to the same final states.
Also, when particle masses are switched on or off, new library files
must be generated (however, masses may be changed between non-zero
values keeping the same process libraries). The best recipe is to 
create a new and separate setup directory in such cases. Otherwise the 
@code{Process} and @code{Results} directories have to be erased:

@example
@code{rm -rf Process/ Results/}
@end example

In either case one has to start over with the whole initialization 
procedure to prepare for the generation of events.



@subsection The example set-up: Z+Jets at the LHC

The setup file ( @code{Run.dat} ) provided in @code{./Examples/V_plus_Jets/LHC_ZJets/} 
can be considered as a standard example to illustrate the generation of fully hadronised 
events in Sherpa, cf. @ref{LHC_ZJets}. Such events will include effects from
parton showering, hadronisation into primary hadrons and their subsequent 
decays into stable hadrons. Moreover, the example chosen here nicely 
demonstrates how Sherpa is used in the context of merging matrix elements 
and parton showers @mycite{Hoeche2009rj}. In addition to the aforementioned 
corrections, this simulation of inclusive Drell-Yan production 
(electron-positron channel) will then include higher-order jet corrections
at the tree level. As a result the transverse-momentum distribution of
the Drell-Yan pair and the individual jet multiplicities as measured by the 
ATLAS and CMS collaborations at the LHC can be well described.

Before event generation, the initialization procedure as described in
@ref{Process selection and initialization} has to be completed. The matrix-element processes
included in the setup are the following:
@iftex
\begin{align*}
  p\,p\;&\to\;{\rm parton parton}\;\to\;e^-e^+\,,\\
  p\,p\;&\to\;{\rm parton parton}\;\to\;e^-e^+\;+\;{\rm parton}\,,\\
  p\,p\;&\to\;{\rm parton parton}\;\to\;e^-e^+\;+\;{\rm parton}\;+\;{\rm parton}\,,\\
  \ldots\;\;.
\end{align*}
@end iftex
@ifnottex
@example
  proton proton -> parton parton -> electron positron + up to four partons
@end example
@end ifnottex

In the @code{(processes)} part of the steering file this translates into
@verbatim
  Process 93 93 -> 11 -11 93{4}
  Order_EW 2;
  CKKW sqr(30/E_CMS)
  End process;
@end verbatim
The physics model for these processes is the Standard Model (@option{SM}) 
which is the default setting of the parameter @code{MODEL} and is therefore 
not set explicitly. Fixing the order of electroweak couplings to @option{2}, 
matrix elements of all partonic subprocesses for Drell-Yan production without 
any and with up to two extra QCD parton emissions will be generated.
Proton--proton collisions are considered at beam energies of 3.5 TeV. 
The default PDF used by Sherpa is CT10. Model parameters and couplings can 
be set in section @code{(run)} of @code{Run.dat}. Similarly, the way 
couplings are treated can be defined. As no options are set the default 
parameters and scale setting procedures are used. 

The QCD radiation matrix elements have to be
regularised to obtain meaningful cross sections. This is achieved by
specifying @samp{CKKW sqr(30/E_CMS)} in the @code{(processes)} part of
@code{Run.dat}. Simultaneously, this tag initiates the ME-PS merging procedure.
To eventually obtain fully hadronized events, the @code{FRAGMENTATION} tag
has been left on it's default setting @option{Ahadic} (and therefore been 
omitted from the run card), which will run Sherpa's cluster hadronisation, 
and the tag @code{DECAYMODEL} has it's default setting @option{Hadrons}, 
which will run Sherpa's hadron decays. Additionally corrections owing to 
photon emissions are taken into account.

To run this example set-up, use the 
@example
@code{<prefix>/bin/Sherpa} 
@end example
command as descibed in @ref{Running Sherpa}. Sherpa displays some output 
as it runs. At the start of the run, Sherpa initializes the relevant model, 
and displays a table of particles, with their @ref{PDG codes} and some 
properties. It also displays the @ref{Particle containers}, and their 
contents. The other relevant parts of Sherpa are initialized, including 
the matrix element generator(s). The Sherpa output will look like:

@smallformat
@verbatim
Initialized the beams Monochromatic*Monochromatic
PDF set 'ct10' loaded for beam 1 (P+).
PDF set 'ct10' loaded for beam 2 (P+).
Initialized the ISR: (SF)*(SF)
Initialize the Standard Model from  / Model.dat
One_Running_AlphaS::One_Running_AlphaS() {
  Setting \alpha_s according to PDF
  perturbative order 1
  \alpha_s(M_Z) = 0.118
}
One_Running_AlphaS::One_Running_AlphaS() {
  Setting \alpha_s according to PDF
  perturbative order 1
  \alpha_s(M_Z) = 0.118
}
Initialized the Soft_Collision_Handler.
Init shower for 1.
CS_Shower::CS_Shower(): Set core m_T mode 0
Shower::Shower(asfacs: IS = 0.73, FS = 1.38)
Init shower for 2.
CS_Shower::CS_Shower(): Set core m_T mode 0
Shower::Shower(asfacs: IS = 0.73, FS = 1.38)
Initialized the Shower_Handler.
+----------------------------------+
|                                  |
|      CCC  OOO  M   M I X   X     |
|     C    O   O MM MM I  X X      |
|     C    O   O M M M I   X       |
|     C    O   O M   M I  X X      |
|      CCC  OOO  M   M I X   X     |
|                                  |
+==================================+
|  Color dressed  Matrix Elements  |
|     http://comix.freacafe.de     |
|   please cite  JHEP12(2008)039   |
+----------------------------------+
Matrix_Element_Handler::BuildProcesses(): Looking for processes . done ( 23252 kB, 0s ).
Matrix_Element_Handler::InitializeProcesses(): Performing tests . done ( 23252 kB, 0s ).
Initialized the Matrix_Element_Handler for the hard processes.
Initialized the Beam_Remnant_Handler.
Hadron_Init::Init(): Initializing kf table for hadrons.
Initialized the Fragmentation_Handler.
Initialized the Soft_Photon_Handler.
Hadron_Decay_Map::Read:   Initializing HadronDecays.dat. This may take some time.
Initialized the Hadron_Decay_Handler, Decay model = Hadrons
R@end verbatim
@end smallformat

Then Sherpa will start to integrate the cross sections. The output will look 
like:
@smallformat
@verbatim
Process_Group::CalculateTotalXSec(): Calculate xs for '2_2__j__j__e-__e+' (Comix)
Starting the calculation at 11:58:56. Lean back and enjoy ... .
822.035 pb +- ( 16.9011 pb = 2.05601 % ) 5000 ( 11437 -> 43.7 % )
full optimization:  ( 0s elapsed / 22s left ) [11:58:56]   
841.859 pb +- ( 11.6106 pb = 1.37916 % ) 10000 ( 18153 -> 74.4 % )
full optimization:  ( 0s elapsed / 21s left ) [11:58:57]   
...
@end verbatim
@end smallformat
The first line here displays the process which is being calculated. In this 
example, the integration is for the 2->2 process, parton, parton -> electron, 
positron. The matrix element generator used is displayed after the process. 
As the integration progresses, summary lines are displayed, like the one 
shown above. The current estimate of the cross section is displayed, 
along with its statistical error estimate. The number of phase space points
calculated is displayed after this (@option{10000} in this example), and the 
efficiency is displayed after that. On the line below, the time elapsed is
shown, and an estimate of the total time till the optimisation is complete. 
In square brackets is an output of the system clock.

When the integration is complete, the output will look like:

@smallformat
@verbatim
...
852.77 pb +- ( 0.337249 pb = 0.0395475 % ) 300000 ( 313178 -> 98.8 % )
integration time:  ( 19s elapsed / 0s left ) [12:01:35]   
852.636 pb +- ( 0.330831 pb = 0.038801 % ) 310000 ( 323289 -> 98.8 % )
integration time:  ( 19s elapsed / 0s left ) [12:01:35]   
2_2__j__j__e-__e+ : 852.636 pb +- ( 0.330831 pb = 0.038801 % )  exp. eff: 13.4945 %
  reduce max for 2_2__j__j__e-__e+ to 0.607545 ( eps = 0.001 ) 
@end verbatim
@end smallformat
with the final cross section result and its statistical error displayed.

Sherpa will then move on to integrate the other processes specified in the
run card.

When the integration is complete, the event generation will start. 
As the events are being generated, Sherpa will display a summary line stating
how many events have been generated, and an estimate of how long it will take.
When the event generation is complete, Sherpa's 
output looks like:

@smallformat
@verbatim
...
Event 10000 ( 58 s total )                                         
In Event_Handler::Finish : Summarizing the run may take some time.
+----------------------------------------------------+
|                                                    |
|  Total XS is 900.147 pb +- ( 8.9259 pb = 0.99 % )  |
|                                                    |
+----------------------------------------------------+
@end verbatim
@end smallformat

A summary of the number of events generated is displayed, with the 
total cross section for the process. 

The generated events are not stored into a file by
default; for details on how to store the events see 
@ref{Event output formats}.


@subsection Parton-level event generation with Sherpa

Sherpa has its own tree-level matrix-element generators called AMEGIC++ and Comix. 
Furthermore, with the module PHASIC++, sophisticated and
robust tools for phase-space integration are provided. Therefore
Sherpa obviously can be used as a cross-section integrator. Because
of the way Monte Carlo integration is accomplished, this immediately
allows for parton-level event generation. Taking the @code{LHC_ZJets}
setup, users have to modify just a few settings in @code{Run.dat} and
would arrive at a parton-level generation for the process gluon down-quark to electron 
antineutrino and up-quark, to name an example. When, for instance, the
options ``@code{EVENTS=0 OUTPUT=2}'' are added to the command line,
a pure cross-section integration for that process would be obtained
with the results plus integration errors written to the screen.

For the example, the @code{(processes)} section alters to
@verbatim
  Process : 21 1 -> 11 -11 1
  Order_EW 2
  End process
@end verbatim
and under the assumption to start afresh, the initialization procedure has
to be followed as before.
Picking the same collider environment as in the previous
example there are only two more changes before the @code{Run.dat} file
is ready for the calculation of the hadronic cross section of the
process g d to e- e+ d at LHC and subsequent
parton-level event generation with Sherpa. These changes read
@code{SHOWER_GENERATOR=None}, to switch off parton showering, 
@code{FRAGMENTATION=Off}, to do so for the hadronisation effects, 
@code{MI_HANDLER=None}, to switch off multiparton interactions, and
@code{ME_QED=Off}, to switch off resummed QED corrections onto the 
Z -> e- e+ decay. Additionally, the non-perturbative intrinsic transverse 
momentum may be wished to not be taken into account, therefore set 
@code{BEAM_REMNANTS=0;}.

@subsection Running Sherpa with AMEGIC++
@anchor{Running Sherpa with AMEGIC++}

When Sherpa is run using the matrix element generator
AMEGIC++, it is necessary to run it twice. During the first run 
(the initialization run) Feynman diagrams for the hard processes are 
constructed and translated into helicity amplitudes. Furthermore 
suitable phase-space mappings are produced. The amplitudes and 
corresponding integration channels are written to disk as C++ 
sourcecode, placed in a subdirectory called @code{Process}. The 
initialization run is started using the standard Sherpa executable, 
as decribed in @ref{Running Sherpa}. The relevant command is

@example
@code{<prefix>/bin/Sherpa} 
@end example

The initialization run stops with the message "New libraries created. Please 
compile.", which is nothing but the request to carry
out the compilation and linking procedure for the generated
matrix-element libraries. The @code{makelibs} script, provided for this
purpose and created in the working directory, must be invoked by the user
(see @code{./makelibs -h} for help):

@example
./makelibs
@end example

Note that the following tools have to be available for this step:
@code{autoconf}, @code{automake} and @code{libtool}.

Alternatively, if @uref{http://www.scons.org/,,scons} is installed, 
you may invoke @kbd{<prefix>/bin/make2scons} and run @kbd{scons install}.

Afterwards Sherpa can be restarted using the same command as
before. In this run (the generation run) the cross sections of the hard
processes are evaluated. Simultaneously the
integration over phase space is optimized to arrive at an
efficient event generation.

